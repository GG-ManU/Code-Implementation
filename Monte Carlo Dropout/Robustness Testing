import numpy as np
from scipy.stats import bernoulli
from keras.losses import categorical_crossentropy
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

def add_gaussian_noise(images, noise_factor):
    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
    return np.clip(noisy_images, 0., 1.)

def add_salt_and_pepper_noise(images, noise_factor):
    salt_vs_pepper = 0.5
    noisy_images = np.copy(images)
    random_mask = np.random.rand(*images.shape) < noise_factor
    salt_mask = np.random.rand(*images.shape) < salt_vs_pepper
    noisy_images[random_mask & salt_mask] = 1.0
    noisy_images[random_mask & ~salt_mask] = 0.0
    return noisy_images

def random_erasing(images, p, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3):
    noisy_images = np.copy(images)
    for i in range(images.shape[0]):
        if np.random.rand() < p:
            img = noisy_images[i]
            h, w, c = img.shape
            S = h * w
            S_e = np.random.uniform(s_l, s_h) * S
            r_e = np.random.uniform(r_1, r_2)
            H_e = int(np.sqrt(S_e * r_e))
            W_e = int(np.sqrt(S_e / r_e))
            x = np.random.randint(0, w)
            y = np.random.randint(0, h)
            if x + W_e > w:
                W_e = w - x
            if y + H_e > h:
                H_e = h - y
            img[y:y+H_e, x:x+W_e, :] = np.random.rand()
    return noisy_images

def evaluate_model(model, x_test, y_test, noise_type, noise_levels):
    accuracies = []
    log_losses = []
    
    for noise_level in noise_levels:
        if noise_type == 'gaussian':
            noisy_x_test = add_gaussian_noise(x_test, noise_level)
        elif noise_type == 'salt_and_pepper':
            noisy_x_test = add_salt_and_pepper_noise(x_test, noise_level)
        elif noise_type == 'random_erasing':
            noisy_x_test = random_erasing(x_test, noise_level)
        else:
            raise ValueError("Invalid noise type")
        
        predictions = model.predict(noisy_x_test)
        accuracy = accuracy_score(y_test.argmax(axis=1), predictions.argmax(axis=1))
        log_loss = categorical_crossentropy(y_test, predictions).numpy().mean()
        
        accuracies.append(accuracy)
        log_losses.append(log_loss)
    
    return accuracies, log_losses

# Run robustness tests with 5% increments
noise_levels = np.arange(0, 0.55, 0.05)  # 0% to 50% with 5% increments
noise_types = ['gaussian', 'salt_and_pepper', 'random_erasing']

results = {}

for noise_type in noise_types:
    accuracies, log_losses = evaluate_model(model, x_test, y_test, noise_type, noise_levels)
    results[noise_type] = {'accuracies': accuracies, 'log_losses': log_losses}

# Plot results
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

for noise_type in noise_types:
    ax1.plot(noise_levels * 100, results[noise_type]['accuracies'], marker='o', label=noise_type)
ax1.set_xlabel('Noise Level (%)')
ax1.set_ylabel('Accuracy')
ax1.set_title('Accuracy vs Noise Level')
ax1.legend()
ax1.set_xticks(noise_levels * 100)
ax1.grid(True)

for noise_type in noise_types:
    ax2.plot(noise_levels * 100, results[noise_type]['log_losses'], marker='o', label=noise_type)
ax2.set_xlabel('Noise Level (%)')
ax2.set_ylabel('Log Loss')
ax2.set_title('Log Loss vs Noise Level')
ax2.legend()
ax2.set_xticks(noise_levels * 100)
ax2.grid(True)

plt.tight_layout()
plt.show()

# Print results
for noise_type in noise_types:
    print(f"\nResults for {noise_type} noise:")
    print("Noise Level | Accuracy | Log Loss")
    print("------------|----------|----------")
    for level, acc, loss in zip(noise_levels, results[noise_type]['accuracies'], results[noise_type]['log_losses']):
        print(f"{level*100:11.1f}% | {acc:.4f}   | {loss:.4f}")
